# Workday LLM Data Extraction

This project extracts and enriches LLM request/response data for the Workday tenant by combining CSV evaluation files with detailed S3 logs and ClickHouse metadata.

## Overview

The system extracts `request_params` and `response_message` from S3 storage by:
1. Reading CSV files containing trace_ids from evaluation data
2. Querying ClickHouse to get storage_path for each trace_id
3. Fetching detailed JSON logs from S3 using the storage_path
4. Extracting and saving the enriched data to output files

## Architecture

```
CSV Files → ClickHouse → S3 Storage → Enriched Output
(trace_id)  (storage_path)  (request/response)  (combined data)
```

## Prerequisites

### Environment Setup
- VM with access to ClickHouse, S3, and evaluation data
- Python 3.7+ with required packages
- IAM role with S3 GetObject permissions (no explicit AWS credentials needed)

### Required Python Packages
```bash
pip install pandas boto3 clickhouse-driver python-dotenv tqdm
```

## Directory Structure

```
project/
├── data/
│   └── evaluated/
│       ├── prompt_name_1/
│       │   ├── workday_cx_file1.csv
│       │   └── workday_cx_file2.csv
│       └── prompt_name_2/
│           └── workday_cx_file3.csv
├── s3_enriched_data/          # Output directory (auto-created)
├── .env                       # Environment configuration
├── workday_s3_extractor.py    # Main extraction script
├── test_clickhouse_integration.py  # Test script
└── README.md
```

## Configuration

### 1. Environment Variables (.env file)

```bash
# Tenant Configuration
tenant_id=workday_cx

# ClickHouse Configuration (REQUIRED)
clickhouse_username=your_clickhouse_user
clickhouse_password=your_clickhouse_password

# Note: AWS credentials not needed - VM uses IAM role
```

### 2. Input Data Requirements

**CSV Files must contain:**
- `trace_id`: Unique identifier for each LLM request
- Optional columns: `tenant_id`, `prompt_name`, `created_at`, `namespace`, `storage_path`

**File Naming Convention:**
- Files must start with the tenant_id (e.g., `workday_cx_evaluation.csv`)
- Located in `data/evaluated/<prompt_name>/` directories

## Installation and Setup

### 1. Clone and Setup
```bash
# Navigate to your project directory
cd ~/llm-gateway/prompts-evaluation

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install pandas boto3 clickhouse-driver python-dotenv tqdm
```

### 2. Configure Environment
```bash
# Create .env file
cat > .env << EOF
tenant_id=workday_cx
clickhouse_username=your_username
clickhouse_password=your_password
EOF
```

### 3. Prepare Data Structure
```bash
# Create directory structure
mkdir -p data/evaluated/prompt_name_1
mkdir -p data/evaluated/prompt_name_2

# Place your CSV files in appropriate prompt directories
# Files should be named: workday_cx_*.csv
```

## Usage

### 1. Test Connections
```bash
# Test ClickHouse connectivity and data availability
python test_clickhouse_integration.py
```

Expected output:
```
✅ ClickHouse connection successful
✅ Found X records for tenant_id: workday_cx
📁 Y records have storage_path
📋 Sample trace_ids for testing
```

### 2. List Available Files
```bash
# See what files will be processed
python usage_example.py list
```

### 3. Process All Files
```bash
# Run the main extraction script
python workday_s3_extractor.py
```

### 4. Process Specific File
```bash
# Process a single file
python usage_example.py single data/evaluated/prompt1/workday_cx_sample.csv prompt1
```

## How It Works

### Step 1: File Discovery
- Scans `data/evaluated/` for subdirectories (prompt names)
- Finds CSV files starting with `{tenant_id}*` in each directory
- Example: `workday_cx_evaluation_results.csv`

### Step 2: ClickHouse Lookup
For each CSV row with a trace_id:
```sql
SELECT storage_path, tenant_id, prompt_name, created_at, namespace
FROM llm_analytics.llm_analytics
WHERE trace_id = %(trace_id)s
```

### Step 3: S3 Path Resolution
- **If storage_path exists**: Use it directly
- **If storage_path is NULL**: Construct path as:
  ```
  llm-analytics/{tenant_id}/{prompt_name}/{year}/{month}/{day}/{trace_id}.json
  ```

### Step 4: S3 Data Extraction
- Fetch JSON content from S3
- Extract `request_params` and `response_message` fields
- Handle retry logic (tries next day if file not found)

### Step 5: Output Generation
- Combines original CSV data with extracted S3 data
- Saves to `s3_enriched_data/{prompt_name}_enriched_{filename}.csv`
- Includes debugging columns: `s3_key_used`, `clickhouse_storage_path`, `s3_fetch_success`

## Output Format

The enriched CSV files contain all original columns plus:

| Column | Description |
|--------|-------------|
| `request_params` | JSON string of LLM request parameters |
| `response_message` | JSON string of LLM response |
| `s3_fetch_success` | Boolean indicating successful S3 fetch |
| `s3_key_used` | S3 path that was accessed |
| `clickhouse_storage_path` | Storage path from ClickHouse (for debugging) |

## Troubleshooting

### ClickHouse Connection Issues
```bash
# Test basic connectivity
python -c "
from clickhouse_driver import Client
client = Client(host='central-clickhouse.aisera.cloud', port=9000)
print(client.execute('SELECT 1'))
"
```

### S3 Access Issues
```bash
# Test S3 access with AWS CLI
aws s3 ls s3://aiseratenants-prod1/llm-analytics/workday_cx/ --recursive | head -5
```

### Common Error Solutions

**"No data found for trace_id"**
- Check if trace_id exists in ClickHouse
- Verify tenant_id configuration matches ClickHouse data

**"Access denied for S3 key"**
- Verify VM has proper IAM role with S3 GetObject permissions
- Check bucket name mapping in `get_bucket_name_from_namespace()`

**"Key not found in S3"**
- Script automatically tries next day (+1 day)
- Check if storage_path or constructed path is correct

## Performance Considerations

- **Processing Speed**: ~10 seconds per row (includes ClickHouse query + S3 fetch)
- **Batch Processing**: Processes files sequentially, rows within file sequentially
- **Rate Limiting**: 0.1 second delay between requests to avoid overwhelming services
- **Memory Usage**: Loads entire CSV into memory (suitable for files up to ~100K rows)

## Development Environment Setup

For setting up in dev environment:

1. **Request from DevOps:**
   - ClickHouse access to dev analytics database
   - S3 access to dev buckets containing workday data
   - IAM role with appropriate permissions

2. **Update Configuration:**
   ```bash
   # Dev environment variables
   tenant_id=workday_dev  # or appropriate dev tenant name
   environment=dev        # if bucket naming differs
   ```

3. **Test with Dev Data:**
   ```bash
   python test_clickhouse_integration.py
   ```

## Security Notes

- No AWS credentials stored in code or config files
- Uses VM's IAM role for S3 access
- ClickHouse credentials stored in .env file (add to .gitignore)
- Read-only access to both ClickHouse and S3

## Monitoring and Logging

The script provides detailed logging:
- **[INFO]**: Processing status and progress
- **[WARN]**: Missing data or fallback usage
- **[ERROR]**: Connection or processing errors
- **[SUMMARY]**: Final statistics per file

Monitor the success rate - typical rates:
- **ClickHouse hits**: 80-95% (depends on data completeness)
- **S3 fetch success**: 70-90% (some trace_ids may not have S3 logs)

## Support

For issues:
1. Check the troubleshooting section
2. Review log output for specific error messages
3. Test individual components (ClickHouse, S3) separately
4. Verify data availability in source systems